{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo\n",
    "In Dynamic Programming algorithms we had a full model of the problem, but usually it's not the case in real life problems. In DP we didn't play the game.<br>\n",
    "In monte carlo the agent will learn purely from experience. We perform the update at the end of the episode, so it's not fully online.<br>\n",
    "In monte carlo we play the game several times and we log the sequence of visited states and rewards. Then we compute the expected returns as the sum of the future rewards (eventually discounted). Finally, when we have several returns for each state, gathered from different runs of the game, we compute the sample mean of the returns to estimate the expected return in each state.<br>\n",
    "If we visit the same state more than once, we ha two options:\n",
    "- First-visit Monte Carlo: where we only consider the first time we see the state\n",
    "- Every-visit Monte Carlo: where we consider the rewards of all the repeated visits in one state\n",
    "\n",
    "It's proven that both methods brings to the same solution.<br>\n",
    "We can also compute the sample mean from the previous mean, without the need to keep a log of all the past returns. For non-stationary problems we could also use a moving average.<br>\n",
    "Monte Carlo is better than DP when the state space is too big to be completely explored, in fact we use only the states that have been visited by the agent during the simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from grid_world import negative_grid, standard_grid\n",
    "plt.rcParams['figure.figsize'] = (15,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 10e-4\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "def print_values(V, g):\n",
    "    for i in range(g.width):\n",
    "        print('-----------------------')\n",
    "        for j in range(g.height):\n",
    "            v = V.get((i, j), 0)\n",
    "            if v >= 0:\n",
    "                print(' {0:.2f}'.format(v), end = ' ')\n",
    "            else:\n",
    "                print('{0:.2f}'.format(v), end = ' ')\n",
    "        print()\n",
    "    print('-----------------------')\n",
    "\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.width):\n",
    "        print('---------------')\n",
    "        for j in range(g.height):\n",
    "            p = P.get((i, j), ' ')\n",
    "            print(' ' + p + ' ', end = ' ')\n",
    "        print()\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation - Monte Carlo\n",
    "Exploring start method: we start from a random state in roder to try to visit all the states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_game(grid, policy):\n",
    "    ## Exploring start method\n",
    "    start_states = list(grid.actions.keys())\n",
    "    start = start_states[np.random.choice(len(start_states))]\n",
    "    grid.set_state(start)\n",
    "    states_and_rewards = [(start, 0)]\n",
    "    s = start\n",
    "    while not grid.game_over():\n",
    "        a = policy[s]\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        states_and_rewards.append((s, r))\n",
    "    \n",
    "    G = 0\n",
    "    states_and_returns = []\n",
    "    first = True\n",
    "    for s, r in states_and_rewards[::-1]:\n",
    "        if not first:\n",
    "            states_and_returns.append((s, G))\n",
    "        else:\n",
    "            first = False\n",
    "        G = r + GAMMA * G\n",
    "    return states_and_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "-----------------------\n",
      " 0.00  0.00  0.00  1.00 \n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -1.00 \n",
      "-----------------------\n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Policy:\n",
      "---------------\n",
      " R   R   R      \n",
      "---------------\n",
      " U       R      \n",
      "---------------\n",
      " U   R   R   U  \n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 100/100 [00:00<00:00, 4163.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value:\n",
      "-----------------------\n",
      " 0.81  0.90  1.00  0.00 \n",
      "-----------------------\n",
      " 0.73  0.00 -1.00  0.00 \n",
      "-----------------------\n",
      " 0.66 -0.81 -0.90 -1.00 \n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "grid = standard_grid()\n",
    "\n",
    "print('Rewards:')\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "states = grid.all_states()\n",
    "V = {s: 0 for s in states}\n",
    "returns = {s: [] for s in states}\n",
    "policy = {(2,0): 'U',\n",
    "         (1,0): 'U',\n",
    "         (0,0): 'R',\n",
    "         (0,1): 'R',\n",
    "         (0,2): 'R',\n",
    "         (1,2): 'R',\n",
    "         (2,1): 'R',\n",
    "         (2,2): 'R',\n",
    "         (2,3): 'U'}\n",
    "print('Policy:')\n",
    "print_policy(policy, grid)\n",
    "grid.policy = policy\n",
    "print()\n",
    "\n",
    "for i in tqdm.tqdm(range(100)):\n",
    "    states_and_returns = play_game(grid, policy)\n",
    "    seen_states = set()\n",
    "    for s, g in states_and_returns:\n",
    "        if not s in seen_states:\n",
    "            returns[s].append(g)\n",
    "            V[s] = np.mean(returns[s])\n",
    "        seen_states.add(s)\n",
    "\n",
    "print('Value:')\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windy Gridworld\n",
    "Here the state transitions are not deterministic, so Monte Carlo can help. The policy will guide the agent to the winning state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "-----------------------\n",
      " 0.00  0.00  0.00  1.00 \n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -1.00 \n",
      "-----------------------\n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Random policy:\n",
      "---------------\n",
      " R   R   R      \n",
      "---------------\n",
      " U       U      \n",
      "---------------\n",
      " U   R   U   L  \n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 5000/5000 [00:02<00:00, 2177.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value:\n",
      "-----------------------\n",
      " 0.58  0.73  0.86  0.00 \n",
      "-----------------------\n",
      " 0.43  0.00  0.25  0.00 \n",
      "-----------------------\n",
      " 0.29  0.15  0.13 -0.19 \n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "def windy_action(a):\n",
    "    p = np.random.rand()\n",
    "    if p < 0.5:\n",
    "        return a\n",
    "    else:\n",
    "        l = list(ALL_POSSIBLE_ACTIONS)\n",
    "        l.remove(a)\n",
    "        return np.random.choice(l)\n",
    "    \n",
    "def play_game(grid, policy):\n",
    "    ## Exploring start method\n",
    "    start_states = list(grid.actions.keys())\n",
    "    start = start_states[np.random.choice(len(start_states))]\n",
    "    grid.set_state(start)\n",
    "    states_and_rewards = [(start, 0)]\n",
    "    s = start\n",
    "    while not grid.game_over():\n",
    "        a = windy_action(policy[s])\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        states_and_rewards.append((s, r))\n",
    "    \n",
    "    G = 0\n",
    "    states_and_returns = []\n",
    "    first = True\n",
    "    for s, r in states_and_rewards[::-1]:\n",
    "        if not first:\n",
    "            states_and_returns.append((s, G))\n",
    "        else:\n",
    "            first = False\n",
    "        G = r + GAMMA * G\n",
    "    return states_and_returns\n",
    "\n",
    "\n",
    "grid = standard_grid()\n",
    "print('Rewards:')\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "states = grid.all_states()\n",
    "V = {s: 0 for s in states}\n",
    "returns = {s: [] for s in states}\n",
    "policy = {(2,0): 'U',\n",
    "         (1,0): 'U',\n",
    "         (0,0): 'R',\n",
    "         (0,1): 'R',\n",
    "         (0,2): 'R',\n",
    "         (1,2): 'U',\n",
    "         (2,1): 'R',\n",
    "         (2,2): 'U',\n",
    "         (2,3): 'L'}\n",
    "print('Random policy:')\n",
    "print_policy(policy, grid)\n",
    "grid.policy = policy\n",
    "print()\n",
    "\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    states_and_returns = play_game(grid, policy)\n",
    "    seen_states = set()\n",
    "    for s, g in states_and_returns:\n",
    "        if not s in seen_states:\n",
    "            returns[s].append(g)\n",
    "            V[s] = np.mean(returns[s])\n",
    "        seen_states.add(s)\n",
    "\n",
    "print('Value:')\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control problem - Find the optimal policy\n",
    "When we are in a state s, in order to choose the best action we should know the V of the next states and pick the max. We can do this by using the Q(s, a) function.<br>\n",
    "By using Q instead of V, we have |S|x|A| values to aproximate instead of |S|, so we need more runs of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "-----------------------\n",
      " 0.00  0.00  0.00  1.00 \n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -1.00 \n",
      "-----------------------\n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Random policy:\n",
      "---------------\n",
      " R   R   R      \n",
      "---------------\n",
      " D       D      \n",
      "---------------\n",
      " R   R   R   U  \n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 5000/5000 [00:01<00:00, 2722.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy\n",
      "---------------\n",
      " R   R   R      \n",
      "---------------\n",
      " U       U      \n",
      "---------------\n",
      " U   R   U   L  \n",
      "---------------\n",
      "\n",
      "Value function:\n",
      "-----------------------\n",
      " 0.81  0.90  1.00  0.00 \n",
      "-----------------------\n",
      " 0.73  0.00  0.90  0.00 \n",
      "-----------------------\n",
      " 0.66  0.73  0.81  0.73 \n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "def opposite_actions(a1, a2):\n",
    "    comb = [('U', 'D'), ('D', 'U'), ('R', 'L'), ('L', 'R')]\n",
    "    for c in comb:\n",
    "        if a1 == c[0] and a2 == c[1]:\n",
    "            return True\n",
    "\n",
    "def play_game(grid, policy):\n",
    "    ## Exploring start method\n",
    "    start_states = list(grid.actions.keys())\n",
    "    start = start_states[np.random.choice(len(start_states))]\n",
    "    a = np.random.choice(grid.actions[start])\n",
    "    grid.set_state(start)\n",
    "    states_and_rewards = [(start, a, 0)]\n",
    "    s = start\n",
    "    while True:\n",
    "        old_s = grid.current_state()\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        if old_s == s:\n",
    "            states_and_rewards.append((s, None, -100))\n",
    "            break\n",
    "        elif grid.game_over():\n",
    "            states_and_rewards.append((s, None, r))\n",
    "            break\n",
    "        elif opposite_actions(policy[s], a):\n",
    "            l = list(grid.actions[s])\n",
    "            l.remove(policy[s])\n",
    "            a = np.random.choice(l)\n",
    "            states_and_rewards.append((s, a, r))\n",
    "        else:\n",
    "            a = policy[s]\n",
    "            states_and_rewards.append((s, a, r))\n",
    "    \n",
    "    G = 0\n",
    "    states_and_returns = []\n",
    "    first = True\n",
    "    for s, a, r in states_and_rewards[::-1]:\n",
    "        if not first:\n",
    "            states_and_returns.append((s, a, G))\n",
    "        else:\n",
    "            first = False\n",
    "        G = r + GAMMA * G\n",
    "    return states_and_returns[::-1]\n",
    "\n",
    "\n",
    "grid = standard_grid()\n",
    "print('Rewards:')\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "states = grid.all_states()\n",
    "Q = {(s, a): 0 for s in states for a in ALL_POSSIBLE_ACTIONS}\n",
    "returns = {(s, a): [] for s in states for a in ALL_POSSIBLE_ACTIONS}\n",
    "policy = {(2,0): 'R',\n",
    "         (1,0): 'D',\n",
    "         (0,0): 'R',\n",
    "         (0,1): 'R',\n",
    "         (0,2): 'R',\n",
    "         (1,2): 'D',\n",
    "         (2,1): 'R',\n",
    "         (2,2): 'R',\n",
    "         (2,3): 'U'}\n",
    "print('Random policy:')\n",
    "print_policy(policy, grid)\n",
    "grid.policy = policy\n",
    "print()\n",
    "\n",
    "for i in tqdm.tqdm(range(5000)):\n",
    "    # policy evaluation\n",
    "    states_and_returns = play_game(grid, policy)\n",
    "    seen_states_actions = set()\n",
    "    for s, a, g in states_and_returns:\n",
    "        if not (s, a) in seen_states_actions:\n",
    "            returns[(s, a)].append(g)\n",
    "            Q[(s, a)] = np.mean(returns[(s, a)])\n",
    "        seen_states_actions.add((s, a))\n",
    "    \n",
    "    # policy improvement\n",
    "    for s in policy.keys():\n",
    "        max_a = policy[s]\n",
    "        for a in grid.actions[s]:\n",
    "            if Q[(s, a)] > Q[(s, max_a)]:\n",
    "                max_a = a\n",
    "        policy[s] = max_a\n",
    "\n",
    "print('Policy')\n",
    "print_policy(policy, grid)\n",
    "\n",
    "V = {s: 0 for s, a in Q}\n",
    "for s, a in Q:\n",
    "    v = Q[(s, a)]\n",
    "    V[s] = v if v > V[s] else V[s]\n",
    "    \n",
    "print('\\nValue function:')\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo control Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "-----------------------\n",
      "-0.10 -0.10 -0.10  1.00 \n",
      "-----------------------\n",
      "-0.10  0.00 -0.10 -1.00 \n",
      "-----------------------\n",
      "-0.10 -0.10 -0.10 -0.10 \n",
      "-----------------------\n",
      "Random policy:\n",
      "---------------\n",
      " R   R   R      \n",
      "---------------\n",
      " D       D      \n",
      "---------------\n",
      " R   R   R   U  \n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 10000/10000 [00:10<00:00, 991.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy\n",
      "---------------\n",
      " R   R   R      \n",
      "---------------\n",
      " U       U      \n",
      "---------------\n",
      " U   R   U   L  \n",
      "---------------\n",
      "\n",
      "Value function:\n",
      "-----------------------\n",
      " 0.58  0.78  1.00  0.00 \n",
      "-----------------------\n",
      " 0.41  0.00  0.78  0.00 \n",
      "-----------------------\n",
      " 0.25  0.35  0.54  0.32 \n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "def random_action(a, eps=0.1):\n",
    "    '''p = [eps / 3 for i in ALL_POSSIBLE_ACTIONS]\n",
    "    p[ALL_POSSIBLE_ACTIONS.index(a)] = 1 - eps\n",
    "    return np.random.choice(ALL_POSSIBLE_ACTIONS, p=p)'''\n",
    "    p = np.random.rand()\n",
    "    if p < 1 - eps:\n",
    "        return a\n",
    "    else:\n",
    "        return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "def opposite_actions(a1, a2):\n",
    "    comb = [('U', 'D'), ('D', 'U'), ('R', 'L'), ('L', 'R')]\n",
    "    for c in comb:\n",
    "        if a1 == c[0] and a2 == c[1]:\n",
    "            return True\n",
    "\n",
    "def play_game(grid, policy):\n",
    "    start = (2,0)\n",
    "    a = random_action(policy[start])\n",
    "    grid.set_state(start)\n",
    "    states_and_rewards = [(start, a, 0)]\n",
    "    s = start\n",
    "    while True:\n",
    "        old_s = grid.current_state()\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        '''if old_s == s:\n",
    "            states_and_rewards.append((s, None, -100))\n",
    "            break'''\n",
    "        if grid.game_over():\n",
    "            states_and_rewards.append((s, None, r))\n",
    "            break\n",
    "        else:\n",
    "            a = random_action(policy[s])\n",
    "            states_and_rewards.append((s, a, r))\n",
    "    \n",
    "    G = 0\n",
    "    states_and_returns = []\n",
    "    first = True\n",
    "    for s, a, r in states_and_rewards[::-1]:\n",
    "        if not first:\n",
    "            states_and_returns.append((s, a, G))\n",
    "        else:\n",
    "            first = False\n",
    "        G = r + GAMMA * G\n",
    "    return states_and_returns[::-1]\n",
    "\n",
    "\n",
    "grid = negative_grid(step_cost=-0.1)\n",
    "print('Rewards:')\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "states = grid.all_states()\n",
    "Q = {(s, a): 0 for s in states for a in ALL_POSSIBLE_ACTIONS}\n",
    "returns = {(s, a): [] for s in states for a in ALL_POSSIBLE_ACTIONS}\n",
    "policy = {(2,0): 'R',\n",
    "         (1,0): 'D',\n",
    "         (0,0): 'R',\n",
    "         (0,1): 'R',\n",
    "         (0,2): 'R',\n",
    "         (1,2): 'D',\n",
    "         (2,1): 'R',\n",
    "         (2,2): 'R',\n",
    "         (2,3): 'U'}\n",
    "print('Random policy:')\n",
    "print_policy(policy, grid)\n",
    "grid.policy = policy\n",
    "print()\n",
    "\n",
    "for i in tqdm.tqdm(range(10000)):\n",
    "    # policy evaluation\n",
    "    states_and_returns = play_game(grid, policy)\n",
    "    seen_states_actions = set()\n",
    "    for s, a, g in states_and_returns:\n",
    "        if not (s, a) in seen_states_actions:\n",
    "            returns[(s, a)].append(g)\n",
    "            Q[(s, a)] = np.mean(returns[(s, a)])\n",
    "        seen_states_actions.add((s, a))\n",
    "    \n",
    "    # policy improvement\n",
    "    for s in policy.keys():\n",
    "        max_a = policy[s]\n",
    "        for a in grid.actions[s]:\n",
    "            if Q[(s, a)] > Q[(s, max_a)]:\n",
    "                max_a = a\n",
    "        policy[s] = max_a\n",
    "\n",
    "print('Policy')\n",
    "print_policy(policy, grid)\n",
    "\n",
    "V = {s: 0 for s, a in Q}\n",
    "for s, a in Q:\n",
    "    v = Q[(s, a)]\n",
    "    V[s] = v if v > V[s] else V[s]\n",
    "    \n",
    "print('\\nValue function:')\n",
    "print_values(V, grid)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
