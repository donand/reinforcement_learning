{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latex\n",
    "# Terminology\n",
    "In tic tac toe game we have a model of the environment, it's not always possible.\n",
    "- Agent\n",
    "- Environment\n",
    "- State: the condition of the environment that the agent can sense\n",
    "- Action: things the agent can do that will affect its state, the action brings to the next state\n",
    "- Reward\n",
    "- Episode: is one run of the game, an agent will take several episodes to be trained\n",
    "- Episodic task: we can play an episode again and again\n",
    "- Coninuous task: it never ends, it's not made of episodes\n",
    "- Terminal state\n",
    "\n",
    "# Value Function\n",
    "It's a measure of the future reward we could get when we are in the current state.<br>\n",
    "For each episode, we update only the states that have been visited by the agent, and we update them backwords. The formula is:\n",
    "$$V(s) = V(s) + \\alpha(V(s^') - V(s))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_state_from_hash(h):\n",
    "    n = to_base_n(h, 3)\n",
    "    n = [0. for i in range(9 - len(n))] + [float(i) for i in n]\n",
    "    n = np.array(n).reshape(3,3)\n",
    "    return n - 1\n",
    "    \n",
    "def state_hash(state):\n",
    "        res = str((state + 1).reshape(9))\n",
    "        res = res[1:-1].split()\n",
    "        res = [s[0] for s in res]\n",
    "        res = ''.join(res)\n",
    "        return int(res, 3)\n",
    "        \n",
    "def to_base_n(n, base):\n",
    "    convert_string = '0123456789ABCDEF'\n",
    "    if n < base:\n",
    "        return convert_string[n]\n",
    "    else:\n",
    "        return to_base_n(n // base, base) + convert_string[n % base]\n",
    "    \n",
    "class Human:\n",
    "    def __init__(self, player_number, name, eps = 0.1, alpha = 0.5, verbose = False):\n",
    "        self.name = name\n",
    "        self.player_number = player_number\n",
    "        self.sign = 1 if player_number == 1 else -1\n",
    "        \n",
    "    def take_action(self, env):\n",
    "        while True:\n",
    "            inp = input('Enter coordinates i,j for the next move: ')\n",
    "            i, j = inp.split(',')\n",
    "            i, j = int(i), int(j)\n",
    "            if i >= 0 and i <= 2 and j >= 0 and j <= 2 and env.state[i,j] == 0:\n",
    "                env.new_action(self, i, j)\n",
    "                break\n",
    "        \n",
    "    def update_state_history(self, state):\n",
    "        pass\n",
    "        \n",
    "    def update_v(self, env):\n",
    "        pass\n",
    "        \n",
    "    def reset_history(self):\n",
    "        pass\n",
    "    \n",
    "    def initialize_V(self, env):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self, player_number, name, eps = 0.1, alpha = 0.5, verbose = False):\n",
    "        self.sign = 1 if player_number == 1 else -1\n",
    "        self.name = name\n",
    "        self.state_history = []\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def take_action(self, env):\n",
    "        r = np.random.rand()\n",
    "        possible_actions = env.get_possible_actions()\n",
    "        if r < self.eps:\n",
    "            if self.verbose:\n",
    "                print('Taking random action')\n",
    "            action = possible_actions[np.random.choice(len(possible_actions))]\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print('Taking greedy action')\n",
    "            action = None\n",
    "            best_value = -1\n",
    "            l = []\n",
    "            for a in possible_actions:\n",
    "                env.new_action(self, a[0], a[1])\n",
    "                state = state_hash(env.get_state())\n",
    "                env.state[a[0], a[1]] = 0\n",
    "                env.next_player = self\n",
    "                l.append(self.v[state])\n",
    "                if self.v[state] > best_value:\n",
    "                    best_value = self.v[state]\n",
    "                    action = a\n",
    "                    best_state = state\n",
    "            if self.verbose:\n",
    "                k = 0\n",
    "                print('-------------')\n",
    "                for i in range(3):\n",
    "                    print('|', end = '')\n",
    "                    for j in range(3):\n",
    "                        if env.state[i,j] == 0:\n",
    "                            symbol = '{0:.2f}'.format(l[k])\n",
    "                            k += 1\n",
    "                        else:\n",
    "                            symbol = env.get_symbol(env.state[i,j])\n",
    "                        print(' {} |'.format(symbol), end = '')\n",
    "                    print()\n",
    "                    print('-------------')\n",
    "                \n",
    "        env.new_action(self, action[0], action[1])\n",
    "        \n",
    "    def update_state_history(self, state):\n",
    "        self.state_history.append(state_hash(state))\n",
    "        \n",
    "    def update_v(self, env):\n",
    "        s_prime = self.state_history[-1]\n",
    "        self.v[s_prime] = env.reward(self)\n",
    "        if self.verbose:\n",
    "            print('Updating the V, last state reward: {}'.format(env.reward(self)))\n",
    "        for s in self.state_history[-2::-1]:\n",
    "            #update the value function V(s) = V(s) + alpha*(V(s') - V(s))\n",
    "            self.v[s] = self.v[s] + self.alpha * (self.v[s_prime] - self.v[s])\n",
    "            s_prime = s\n",
    "        self.reset_history()\n",
    "        \n",
    "    def reset_history(self):\n",
    "        self.state_history = []\n",
    "    \n",
    "    def initialize_V(self, env):\n",
    "        v = np.zeros(env.number_of_states)\n",
    "        for i in range(env.number_of_states):\n",
    "            env.state = get_state_from_hash(i)\n",
    "            winner = env.get_winner()\n",
    "            if env.game_over():\n",
    "                if winner is None or winner.sign == - self.sign:\n",
    "                    v[i] = 0\n",
    "                elif winner.sign == self.sign:\n",
    "                    v[i] = 1\n",
    "            else:\n",
    "                v[i] = 0.5\n",
    "        env.state = np.zeros((3,3))\n",
    "        self.v = v\n",
    "    \n",
    "    \n",
    "class Environment:\n",
    "    def __init__(self, first_player, second_player):\n",
    "        self.state = np.zeros((3,3))\n",
    "        self.p1 = first_player\n",
    "        self.p2 = second_player\n",
    "        self.next_player = first_player\n",
    "        self.number_of_states = 3**9\n",
    "        \n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def get_symbol(self, n):\n",
    "        if n == 1:\n",
    "            return 'x'\n",
    "        elif n == -1:\n",
    "            return 'o'\n",
    "        else:\n",
    "            return ' '\n",
    "        \n",
    "    def draw_board(self):\n",
    "        print('\\nIt\\'s the turn of the player ' + self.next_player.name)\n",
    "        print('-------------')\n",
    "        for i in range(3):\n",
    "            print('|', end = '')\n",
    "            for j in range(3):\n",
    "                print(' {} |'.format(self.get_symbol(self.state[i,j])), end = '')\n",
    "            print()\n",
    "            print('-------------')\n",
    "        \n",
    "    def new_action(self, player, x, y):\n",
    "        if x < 0 or x > 2 or y < 0 or y > 2 or self.state[x,y] != 0:\n",
    "            sys.exit('Invalid action')\n",
    "        if player != self.next_player:\n",
    "            sys.exit('Invalid turn')\n",
    "        self.state[x, y] = player.sign\n",
    "        self.next_player = self.p1 if player == self.p2 else self.p2\n",
    "        \n",
    "    def next_player(self):\n",
    "        return self.next_player\n",
    "    \n",
    "    def get_possible_actions(self):\n",
    "        c = np.where(self.state == 0)\n",
    "        return list(zip(c[0], c[1]))\n",
    "    \n",
    "    def get_winner(self):\n",
    "        winner = None\n",
    "        s = np.sum(self.state, axis = 0)\n",
    "        if len(np.extract(s == -3, s)) > 0 or np.sum(self.state.trace()) == -3 or np.sum(self.state[::-1].trace()) == -3:\n",
    "            winner = self.p2\n",
    "        elif len(np.extract(s == 3, s)) > 0 or np.sum(self.state.trace()) == 3 or np.sum(self.state[::-1].trace()) == 3:\n",
    "            winner = self.p1\n",
    "        s = np.sum(self.state, axis = 1)\n",
    "        if len(np.extract(s == -3, s)) > 0:\n",
    "            winner = self.p2\n",
    "        elif len(np.extract(s == 3, s)) > 0:\n",
    "            winner = self.p1\n",
    "        return winner\n",
    "    \n",
    "    def game_over(self):\n",
    "        return len(np.extract(self.state == 0, self.state)) == 0 or self.get_winner() != None\n",
    "    \n",
    "    def reward(self, player):\n",
    "        if not self.game_over():\n",
    "            return 0\n",
    "        if self.get_winner() == player:\n",
    "            reward = 1\n",
    "        elif self.get_winner() == None:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = -1\n",
    "        return reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.zeros((3,3))\n",
    "        self.next_player = self.p1\n",
    "    \n",
    "    \n",
    "def play_game(p1, p2, env, draw = False):\n",
    "    if draw:\n",
    "            env.draw_board()\n",
    "    while not env.game_over():\n",
    "        current_player = env.next_player  \n",
    "        #print(current_player.name, current_player.sign)\n",
    "        current_player.take_action(env)\n",
    "        \n",
    "        # update the state history\n",
    "        state = env.get_state()\n",
    "        p1.update_state_history(state)\n",
    "        p2.update_state_history(state)\n",
    "        \n",
    "        if draw:\n",
    "            env.draw_board()\n",
    "            \n",
    "    # update the value function\n",
    "    p1.update_v(env)\n",
    "    p2.update_v(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\np1, p2 = Agent(1, 'p1', verbose = True), Agent(2, 'p2')\\nenv = Environment(p1, p2)\\np1.initialize_V(env)\\np2.initialize_V(env)\\nplay_game(p1, p2, env, draw = True)\\nwinner = env.get_winner()\\nif winner is None:\\n    print('The game ended in a draw')\\nelse:\\n    print('The winner is ' + env.get_winner().name)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "p1, p2 = Agent(1, 'p1', verbose = True), Agent(2, 'p2')\n",
    "env = Environment(p1, p2)\n",
    "p1.initialize_V(env)\n",
    "p2.initialize_V(env)\n",
    "play_game(p1, p2, env, draw = True)\n",
    "winner = env.get_winner()\n",
    "if winner is None:\n",
    "    print('The game ended in a draw')\n",
    "else:\n",
    "    print('The winner is ' + env.get_winner().name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/10000\n",
      "Episode 500/10000\n",
      "Episode 1000/10000\n",
      "Episode 1500/10000\n",
      "Episode 2000/10000\n",
      "Episode 2500/10000\n",
      "Episode 3000/10000\n",
      "Episode 3500/10000\n",
      "Episode 4000/10000\n",
      "Episode 4500/10000\n",
      "Episode 5000/10000\n",
      "Episode 5500/10000\n",
      "Episode 6000/10000\n",
      "Episode 6500/10000\n",
      "Episode 7000/10000\n",
      "Episode 7500/10000\n",
      "Episode 8000/10000\n",
      "Episode 8500/10000\n",
      "Episode 9000/10000\n",
      "Episode 9500/10000\n"
     ]
    }
   ],
   "source": [
    "p1_train, p2_train = Agent(1, 'p1_train', verbose = False), Agent(2, 'p2_train', verbose = False)\n",
    "env = Environment(p1_train, p2_train)\n",
    "p1_train.initialize_V(env)\n",
    "p2_train.initialize_V(env)\n",
    "T = 10000\n",
    "for t in range(T):\n",
    "    if t%500 == 0:\n",
    "        print('Episode {}/{}'.format(t, T))\n",
    "    play_game(p1_train, p2_train, Environment(p1_train, p2_train))\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It's the turn of the player Andrea\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "Enter coordinates i,j for the next move: 1,1\n",
      "\n",
      "It's the turn of the player p2_train\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   | x |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "Taking greedy action\n",
      "-------------\n",
      "| -0.18 | -0.72 | 0.15 |\n",
      "-------------\n",
      "| -0.67 | x | -0.84 |\n",
      "-------------\n",
      "| -0.43 | -0.73 | -0.29 |\n",
      "-------------\n",
      "\n",
      "It's the turn of the player Andrea\n",
      "-------------\n",
      "|   |   | o |\n",
      "-------------\n",
      "|   | x |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "Enter coordinates i,j for the next move: 1,2\n",
      "\n",
      "It's the turn of the player p2_train\n",
      "-------------\n",
      "|   |   | o |\n",
      "-------------\n",
      "|   | x | x |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "Taking greedy action\n",
      "-------------\n",
      "| -0.62 | -0.95 | o |\n",
      "-------------\n",
      "| 0.46 | x | x |\n",
      "-------------\n",
      "| -0.95 | -0.62 | -0.68 |\n",
      "-------------\n",
      "\n",
      "It's the turn of the player Andrea\n",
      "-------------\n",
      "|   |   | o |\n",
      "-------------\n",
      "| o | x | x |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "Enter coordinates i,j for the next move: 1,2\n",
      "Enter coordinates i,j for the next move: 2,2\n",
      "\n",
      "It's the turn of the player p2_train\n",
      "-------------\n",
      "|   |   | o |\n",
      "-------------\n",
      "| o | x | x |\n",
      "-------------\n",
      "|   |   | x |\n",
      "-------------\n",
      "Taking greedy action\n",
      "-------------\n",
      "| 0.94 | 0.50 | o |\n",
      "-------------\n",
      "| o | x | x |\n",
      "-------------\n",
      "| -0.62 | -0.25 | x |\n",
      "-------------\n",
      "\n",
      "It's the turn of the player Andrea\n",
      "-------------\n",
      "| o |   | o |\n",
      "-------------\n",
      "| o | x | x |\n",
      "-------------\n",
      "|   |   | x |\n",
      "-------------\n",
      "Enter coordinates i,j for the next move: 0,1\n",
      "\n",
      "It's the turn of the player p2_train\n",
      "-------------\n",
      "| o | x | o |\n",
      "-------------\n",
      "| o | x | x |\n",
      "-------------\n",
      "|   |   | x |\n",
      "-------------\n",
      "Taking greedy action\n",
      "-------------\n",
      "| o | x | o |\n",
      "-------------\n",
      "| o | x | x |\n",
      "-------------\n",
      "| 1.00 | 0.00 | x |\n",
      "-------------\n",
      "\n",
      "It's the turn of the player Andrea\n",
      "-------------\n",
      "| o | x | o |\n",
      "-------------\n",
      "| o | x | x |\n",
      "-------------\n",
      "| o |   | x |\n",
      "-------------\n",
      "Updating the V, last state reward: 1\n",
      "Play again? [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "human = Human(1, 'Andrea')\n",
    "p2_train.verbose = True\n",
    "p2_train.eps = 0\n",
    "env = Environment(human, p2_train)\n",
    "while True:\n",
    "    play_game(p2_train, human, env, draw = True)\n",
    "    env.reset()\n",
    "    answer = input('Play again? [Y/n]: ')\n",
    "    if answer and answer.lower()[0] == 'n':\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
