{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference\n",
    "Unless Monte Carlo we don't have to wait for the episode to finish, we can update the V at every step. In MC we can use moving averages.<br>\n",
    "In MC the source of randomness is that each episode can be played in a different way. In TD(0) we don't know the return, but we estimate it with r + gamma * V(s').<br>\n",
    "We do not require a full model of the environment like DP, we learn from experience and we update only the states that we actually visit.<br>\n",
    "Unlike MC, we don't have to wait for an episode to finish in order to learn, this can be advantageous with very long episodes or it can be used in continuous tasks in which there are no episodes at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from grid_world import standard_grid, negative_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "plt.rcParams['figure.figsize'] = (15,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 10e-4\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "def print_values(V, g):\n",
    "    for i in range(g.width):\n",
    "        print('-----------------------')\n",
    "        for j in range(g.height):\n",
    "            v = V.get((i, j), 0)\n",
    "            if v >= 0:\n",
    "                print(' {0:.2f}'.format(v), end = ' ')\n",
    "            else:\n",
    "                print('{0:.2f}'.format(v), end = ' ')\n",
    "        print()\n",
    "    print('-----------------------')\n",
    "\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.width):\n",
    "        print('---------------')\n",
    "        for j in range(g.height):\n",
    "            p = P.get((i, j), ' ')\n",
    "            print(' ' + p + ' ', end = ' ')\n",
    "        print()\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_action(a, eps=0.1):\n",
    "    p = np.random.rand()\n",
    "    if p < 1 - eps:\n",
    "        return a\n",
    "    else:\n",
    "        return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "    \n",
    "def play_game(grid, policy):\n",
    "    s = (2, 0)\n",
    "    grid.set_state(s)\n",
    "    states_and_rewards = [(s, 0)]\n",
    "    while not grid.game_over():\n",
    "        r = grid.move(random_action(policy[s]))\n",
    "        s = grid.current_state()\n",
    "        states_and_rewards.append((s, r))\n",
    "    return states_and_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Problem - Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "-----------------------\n",
      " 0.00  0.00  0.00  1.00 \n",
      "-----------------------\n",
      " 0.00  0.00  0.00 -1.00 \n",
      "-----------------------\n",
      " 0.00  0.00  0.00  0.00 \n",
      "-----------------------\n",
      "Random policy:\n",
      "---------------\n",
      " R   R   R      \n",
      "---------------\n",
      " U       R      \n",
      "---------------\n",
      " U   R   R   U  \n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 22108.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy\n",
      "---------------\n",
      " R   R   R      \n",
      "---------------\n",
      " U       R      \n",
      "---------------\n",
      " U   R   R   U  \n",
      "---------------\n",
      "\n",
      "Value function:\n",
      "-----------------------\n",
      " 0.74  0.85  0.98  0.00 \n",
      "-----------------------\n",
      " 0.65  0.00 -0.98  0.00 \n",
      "-----------------------\n",
      " 0.50 -0.78 -0.88 -0.99 \n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "grid = standard_grid()\n",
    "print('Rewards:')\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "states = grid.all_states()\n",
    "V = {s: 0 for s in states}\n",
    "returns = {(s, a): [] for s in states for a in ALL_POSSIBLE_ACTIONS}\n",
    "policy = {(2,0): 'U',\n",
    "         (1,0): 'U',\n",
    "         (0,0): 'R',\n",
    "         (0,1): 'R',\n",
    "         (0,2): 'R',\n",
    "         (1,2): 'R',\n",
    "         (2,1): 'R',\n",
    "         (2,2): 'R',\n",
    "         (2,3): 'U'}\n",
    "print('Random policy:')\n",
    "print_policy(policy, grid)\n",
    "grid.policy = policy\n",
    "print()\n",
    "\n",
    "for i in tqdm.tqdm(range(10000)):\n",
    "    # policy evaluation\n",
    "    states_and_returns = play_game(grid, policy)\n",
    "    for j in range(len(states_and_returns) - 1):\n",
    "        s, _ = states_and_returns[j]\n",
    "        s2, r = states_and_returns[j + 1]\n",
    "        V[s] = V[s] + ALPHA * (r + GAMMA * V[s2] - V[s])\n",
    "\n",
    "print('Policy')\n",
    "print_policy(policy, grid)\n",
    "    \n",
    "print('\\nValue function:')\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA - Policy Iteration - Control Problem\n",
    "It's like TD(0) with the exception that we update Q(s,a) and not V(s). In order to perform an update we need the tuple (s,a,r,s',a'), hence the name.\n",
    "\n",
    "$$ Q(s,a) = Q(s,a) + \\alpha * [r + \\gamma Q(s', a') - Q(s,a)] $$\n",
    "\n",
    "We can use decaying epsilon and decaying learning rate. The decaying learning rate can be a problem because the $\\alpha$ decays differently for different parts of the Q, in fact if it decays once per episode, some states will be visited only in future episodes.<br>\n",
    "The solution is to use a decaying learning rate for each state/action pair, based on the number of times that the pair has been visited.\n",
    "\n",
    "$$ \\alpha(s,a) = \\frac{\\alpha_0}{count((s,a))} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "-----------------------\n",
      "-0.10 -0.10 -0.10  1.00 \n",
      "-----------------------\n",
      "-0.10  0.00 -0.10 -1.00 \n",
      "-----------------------\n",
      "-0.10 -0.10 -0.10 -0.10 \n",
      "-----------------------\n",
      "Initial policy:\n",
      "---------------\n",
      " R   R   R      \n",
      "---------------\n",
      " U       U      \n",
      "---------------\n",
      " R   R   U   L  \n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 15208.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final epsilon: 0.045454545454545546\n",
      "\n",
      "Optimal policy\n",
      "---------------\n",
      " R   R   R      \n",
      "---------------\n",
      " U       U      \n",
      "---------------\n",
      " U   R   U   L  \n",
      "---------------\n",
      "\n",
      "Optimal value function:\n",
      "-----------------------\n",
      " 0.60  0.79  1.00  0.00 \n",
      "-----------------------\n",
      " 0.43  0.00  0.79  0.00 \n",
      "-----------------------\n",
      " 0.28  0.37  0.57  0.12 \n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "def max_dict(q):\n",
    "    max_a = 'U'\n",
    "    for a in q.keys():\n",
    "        max_a = a if q[a] > q[max_a] else max_a\n",
    "    return max_a, q[max_a]\n",
    "\n",
    "def alpha(s, a, learning_counter):\n",
    "    learning_counter[(s, a)] += 0.005\n",
    "    return ALPHA / learning_counter[(s, a)]\n",
    "\n",
    "grid = negative_grid(step_cost=-0.1)\n",
    "print('Rewards:')\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "states = grid.all_states()\n",
    "Q = {s: {a: 0 for a in ALL_POSSIBLE_ACTIONS} for s in states}\n",
    "learning_counter = {(s, a): 1 for s in states for a in ALL_POSSIBLE_ACTIONS}\n",
    "print('Initial policy:')\n",
    "print_policy(policy, grid)\n",
    "grid.policy = policy\n",
    "print()\n",
    "\n",
    "t = 1\n",
    "for i in tqdm.tqdm(range(10000)):\n",
    "    if i % 100 == 0:\n",
    "        t += 10e-2\n",
    "    # policy evaluation\n",
    "    s = (2, 0)\n",
    "    grid.set_state(s)\n",
    "    a = max_dict(Q[s])[0]\n",
    "    a = random_action(a, eps=0.5/t)\n",
    "    while not grid.game_over():\n",
    "        r = grid.move(a)\n",
    "        s_prime = grid.current_state()\n",
    "        a_prime = max_dict(Q[s_prime])[0]\n",
    "        a_prime = random_action(a_prime, eps=0.5/t)\n",
    "        Q[s][a] = Q[s][a] + alpha(s, a, learning_counter) * (r + GAMMA * Q[s_prime][a_prime] - Q[s][a])\n",
    "        s = s_prime\n",
    "        a = a_prime\n",
    "        \n",
    "print('Final epsilon: {}\\n'.format(0.5/t))\n",
    "\n",
    "V = {s: 0 for s in Q}\n",
    "for s in grid.actions.keys():\n",
    "    a, max_q = max_dict(Q[s])\n",
    "    policy[s] = a\n",
    "    V[s] = max_q\n",
    "\n",
    "print('Optimal policy')\n",
    "print_policy(policy, grid)\n",
    "    \n",
    "print('\\nOptimal value function:')\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "All the algorithms so far were on-policy algorithms, so the game was played following the current policy by taking the best action. Q-Learning is off-policy, so we can play the game taking random actions and still end up with the optimal policy.<br>\n",
    "The difference wrt SARSA is that we don't have to actually take the action a' with which we update Q(s,a), we can just use it in the update and then take another action. Totally random actions are not a good option because the episodes will take longer to finish, so longer time for the same number of episodes.<br>\n",
    "If we always follow epsilon-greedy policy, then Q-Learning is equivalent to SARSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "-----------------------\n",
      "-0.10 -0.10 -0.10  1.00 \n",
      "-----------------------\n",
      "-0.10  0.00 -0.10 -1.00 \n",
      "-----------------------\n",
      "-0.10 -0.10 -0.10 -0.10 \n",
      "-----------------------\n",
      "Initial policy:\n",
      "---------------\n",
      " R   R   R      \n",
      "---------------\n",
      " U       U      \n",
      "---------------\n",
      " R   R   U   L  \n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 14314.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final epsilon: 0.045454545454545546\n",
      "\n",
      "Optimal policy\n",
      "---------------\n",
      " R   R   R      \n",
      "---------------\n",
      " U       U      \n",
      "---------------\n",
      " R   R   U   L  \n",
      "---------------\n",
      "\n",
      "Optimal value function:\n",
      "-----------------------\n",
      " 0.62  0.80  1.00  0.00 \n",
      "-----------------------\n",
      " 0.46  0.00  0.80  0.00 \n",
      "-----------------------\n",
      " 0.31  0.46  0.62  0.46 \n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "def max_dict(q):\n",
    "    max_a = 'U'\n",
    "    for a in q.keys():\n",
    "        max_a = a if q[a] > q[max_a] else max_a\n",
    "    return max_a, q[max_a]\n",
    "\n",
    "def alpha(s, a, learning_counter):\n",
    "    learning_counter[(s, a)] += 0.005\n",
    "    return ALPHA / learning_counter[(s, a)]\n",
    "\n",
    "grid = negative_grid(step_cost=-0.1)\n",
    "print('Rewards:')\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "states = grid.all_states()\n",
    "Q = {s: {a: 0 for a in ALL_POSSIBLE_ACTIONS} for s in states}\n",
    "learning_counter = {(s, a): 1 for s in states for a in ALL_POSSIBLE_ACTIONS}\n",
    "print('Initial policy:')\n",
    "print_policy(policy, grid)\n",
    "grid.policy = policy\n",
    "print()\n",
    "\n",
    "t = 1\n",
    "for i in tqdm.tqdm(range(10000)):\n",
    "    if i % 100 == 0:\n",
    "        t += 10e-2\n",
    "    # policy evaluation\n",
    "    s = (2, 0)\n",
    "    grid.set_state(s)\n",
    "    a = max_dict(Q[s])[0]\n",
    "    while not grid.game_over():\n",
    "        a = random_action(a, eps=0.5/t)\n",
    "        #a = random_action(a, eps=1)\n",
    "        r = grid.move(a)\n",
    "        s_prime = grid.current_state()\n",
    "        a_prime, q_a_prime = max_dict(Q[s_prime])\n",
    "        Q[s][a] = Q[s][a] + alpha(s, a, learning_counter) * (r + GAMMA * q_a_prime - Q[s][a])\n",
    "        s = s_prime\n",
    "        a = a_prime\n",
    "        \n",
    "print('Final epsilon: {}\\n'.format(0.5/t))\n",
    "\n",
    "V = {s: 0 for s in Q}\n",
    "for s in grid.actions.keys():\n",
    "    a, max_q = max_dict(Q[s])\n",
    "    policy[s] = a\n",
    "    V[s] = max_q\n",
    "\n",
    "print('Optimal policy')\n",
    "print_policy(policy, grid)\n",
    "    \n",
    "print('\\nOptimal value function:')\n",
    "print_values(V, grid)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
