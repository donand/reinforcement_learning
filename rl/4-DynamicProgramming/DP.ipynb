{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Policy Evaluation\n",
    "This solve the prediction problem, e.g. given a policy find the value function.<br>\n",
    "First initialise V(s) to zero for all the states, then loop through all the states and update V(s) until it converges.\n",
    "- Prediction problem: given a policy, find the value function\n",
    "- Control problem: find the optimal policy\n",
    "\n",
    "We are going to implement Iterative Policy Evaluation for two different policies, a random and a deterministic policy.\n",
    "\n",
    "### Random Policy\n",
    "The probability of an action is 1 divided by the number of actions.\n",
    "$$\\pi(a | s) = \\frac{1}{|A|}$$\n",
    "$p(s', r | s, a)$ is only relevant when state transitions are stochastic.\n",
    "\n",
    "### Deterministic Policy\n",
    "Completely deterministic, we expect the values of the value function to be 1 in the path to the goal, and -1 in all other states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from grid_world import standard_grid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 10e-4\n",
    "\n",
    "def print_values(V, g):\n",
    "    for i in range(g.width):\n",
    "        print('-----------------------')\n",
    "        for j in range(g.height):\n",
    "            v = V.get((i, j), 0)\n",
    "            if v >= 0:\n",
    "                print(' {0:.2f}'.format(v), end = ' ')\n",
    "            else:\n",
    "                print('{0:.2f}'.format(v), end = ' ')\n",
    "        print()\n",
    "    print('-----------------------')\n",
    "\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.width):\n",
    "        print('---------------')\n",
    "        for j in range(g.height):\n",
    "            p = P.get((i, j), ' ')\n",
    "            print(' ' + p + ' ', end = ' ')\n",
    "        print()\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Policy - Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value function for uniformly random actions:\n",
      "-----------------------\n",
      "-0.03  0.09  0.22  0.00 \n",
      "-----------------------\n",
      "-0.16  0.00 -0.44  0.00 \n",
      "-----------------------\n",
      "-0.29 -0.41 -0.54 -0.77 \n",
      "-----------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid = standard_grid()\n",
    "states = grid.all_states()\n",
    "V = {s: 0 for s in states}\n",
    "# discount factor gamma\n",
    "gamma = 1\n",
    "\n",
    "while True:\n",
    "    delta = 0\n",
    "    for s in states:\n",
    "        old_v = V[s]\n",
    "        if s in grid.actions:\n",
    "            actions = grid.actions[s]\n",
    "            v = 0\n",
    "            for a in actions:\n",
    "                grid.set_state(s)\n",
    "                r = grid.move(a)\n",
    "                s_prime = grid.current_state()\n",
    "                v += 1 / len(actions) * (r + gamma * V[s_prime])\n",
    "            V[s] = v\n",
    "            delta = max(delta, abs(v - old_v))\n",
    "    if delta < THRESHOLD:\n",
    "        break\n",
    "print('\\nValue function for uniformly random actions:')\n",
    "print_values(V, grid)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic Policy - Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      " R   R   R      \n",
      "---------------\n",
      " U       R      \n",
      "---------------\n",
      " U   R   R   U  \n",
      "---------------\n",
      "\n",
      "Value function for deterministic policy:\n",
      "-----------------------\n",
      " 0.81  0.90  1.00  0.00 \n",
      "-----------------------\n",
      " 0.73  0.00 -1.00  0.00 \n",
      "-----------------------\n",
      " 0.66 -0.81 -0.90 -1.00 \n",
      "-----------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy = {(2,0): 'U',\n",
    "         (1,0): 'U',\n",
    "         (0,0): 'R',\n",
    "         (0,1): 'R',\n",
    "         (0,2): 'R',\n",
    "         (1,2): 'R',\n",
    "         (2,1): 'R',\n",
    "         (2,2): 'R',\n",
    "         (2,3): 'U'}\n",
    "print_policy(policy, grid)\n",
    "grid.policy = policy\n",
    "\n",
    "V_det = {s: 0 for s in states}\n",
    "gamma = 0.9\n",
    "\n",
    "while True:\n",
    "    delta = 0\n",
    "    for s in states:\n",
    "        old_v = V_det[s]\n",
    "        if not grid.is_terminal(s):\n",
    "            grid.set_state(s)\n",
    "            a = grid.policy[s]\n",
    "            r = grid.move(a)\n",
    "            s_prime = grid.current_state()\n",
    "            V_det[s] = 1 * (r + gamma * V_det[s_prime])\n",
    "            delta = max(delta, abs(V_det[s] - old_v))\n",
    "    if delta < THRESHOLD:\n",
    "        break\n",
    "print('\\nValue function for deterministic policy:')\n",
    "print_values(V_det, grid)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "We alternate Iterative Policy Evaluation and Policy Improvement.<br>\n",
    "First, we initialise both V and $\\pi$, then we perform the Policy Evaluation to find the V(s), and finally we perform Policy Improvement. If the policy changes in the last step, then we perform again the Policy Evaluation because the V(s) is changed, then Policy Improvement and so on.\n",
    "- Initialise V(s) and $\\pi(s)$\n",
    "- Policy Evaluation to find V(s)\n",
    "- Policy Improvement to find a better policy $\\pi(s)$\n",
    "- If the policy is changed, got o step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "\n",
      "-----------------------\n",
      "-0.10 -0.10 -0.10  1.00 \n",
      "-----------------------\n",
      "-0.10  0.00 -0.10 -1.00 \n",
      "-----------------------\n",
      "-0.10 -0.10 -0.10 -0.10 \n",
      "-----------------------\n",
      "Random policy:\n",
      "---------------\n",
      " D   L   R      \n",
      "---------------\n",
      " D       R      \n",
      "---------------\n",
      " U   L   U   L  \n",
      "---------------\n",
      "\n",
      "\n",
      "Value function: \n",
      "-----------------------\n",
      " 0.62  0.80  1.00  0.00 \n",
      "-----------------------\n",
      " 0.46  0.00  0.80  0.00 \n",
      "-----------------------\n",
      " 0.31  0.46  0.62  0.46 \n",
      "-----------------------\n",
      "\n",
      "Optimal Policy:\n",
      "---------------\n",
      " R   R   R      \n",
      "---------------\n",
      " U       U      \n",
      "---------------\n",
      " U   R   U   L  \n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "from grid_world import negative_grid\n",
    "\n",
    "grid = negative_grid()\n",
    "print('Rewards:\\n')\n",
    "print_values(grid.rewards, grid)\n",
    "states = grid.all_states()\n",
    "\n",
    "policy = {s: grid.actions[s][np.random.choice(len(grid.actions[s]))] for s in states if s in grid.actions}\n",
    "V = {s: 0 for s in states}\n",
    "print('Random policy:')\n",
    "print_policy(policy, grid)\n",
    "print()\n",
    "\n",
    "gamma = 0.9\n",
    "changed = True\n",
    "while changed:\n",
    "    # Perform Policy Evaluation\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "            if s in grid.actions:\n",
    "                grid.set_state(s)\n",
    "                r = grid.move(policy[s])\n",
    "                s_prime = grid.current_state()\n",
    "                V[s] = 1 * (r + gamma * V[s_prime])\n",
    "                delta = max(delta, V[s] - old_v)\n",
    "        if delta < THRESHOLD:\n",
    "            break\n",
    "    \n",
    "    # Perform Policy Improvement\n",
    "    changed = False\n",
    "    for s in states:\n",
    "        if s in grid.actions:\n",
    "            old_a = policy[s]\n",
    "            possible_actions = grid.actions[s]\n",
    "            best_a = old_a\n",
    "            for a in possible_actions:\n",
    "                grid.set_state(s)\n",
    "                r = grid.move(a)\n",
    "                s_prime = grid.current_state()\n",
    "                v = r + gamma * V[s_prime]\n",
    "                best_a = a if v > V[s] else best_a\n",
    "            policy[s] = best_a\n",
    "            if best_a != old_a:\n",
    "                changed = True\n",
    "\n",
    "print('\\nValue function: ')\n",
    "print_values(V, grid)\n",
    "print('\\nOptimal Policy:')\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration in Windy Gridworld\n",
    "Now the state transitions are not deterministic, so when the agent wants to go up it goes up with a probability of 0.5, and it goes left, right or down with a probability of $\\frac{0.5}{3}$.<br>\n",
    "So we have to consider the $p(s', r | s, a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "\n",
      "-----------------------\n",
      "-0.30 -0.30 -0.30  1.00 \n",
      "-----------------------\n",
      "-0.30  0.00 -0.30 -1.00 \n",
      "-----------------------\n",
      "-0.30 -0.30 -0.30 -0.30 \n",
      "-----------------------\n",
      "Random policy:\n",
      "---------------\n",
      " R   L   D      \n",
      "---------------\n",
      " U       D      \n",
      "---------------\n",
      " R   L   U   L  \n",
      "---------------\n",
      "\n",
      "\n",
      "Value function: \n",
      "-----------------------\n",
      "-1.08 -0.52  0.22  0.00 \n",
      "-----------------------\n",
      "-1.49  0.00 -0.57  0.00 \n",
      "-----------------------\n",
      "-1.72 -1.53 -1.13 -1.17 \n",
      "-----------------------\n",
      "\n",
      "Optimal Policy:\n",
      "---------------\n",
      " R   R   R      \n",
      "---------------\n",
      " U       U      \n",
      "---------------\n",
      " U   R   U   U  \n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "from grid_world import negative_grid\n",
    "\n",
    "POSSIBLE_ACTIONS = ['U', 'D', 'L', 'R']\n",
    "\n",
    "grid = negative_grid(step_cost=-0.3)\n",
    "print('Rewards:\\n')\n",
    "print_values(grid.rewards, grid)\n",
    "states = grid.all_states()\n",
    "\n",
    "policy = {s: grid.actions[s][np.random.choice(len(grid.actions[s]))] for s in states if s in grid.actions}\n",
    "V = {s: 0 for s in states}\n",
    "print('Random policy:')\n",
    "print_policy(policy, grid)\n",
    "print()\n",
    "\n",
    "gamma = 0.9\n",
    "changed = True\n",
    "while changed:\n",
    "    # Perform Policy Evaluation\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "            new_v = 0\n",
    "            if s in grid.actions:\n",
    "                for a in POSSIBLE_ACTIONS:\n",
    "                    if a == policy[s]:\n",
    "                        p = 0.5\n",
    "                    else:\n",
    "                        p = 0.5 / 3\n",
    "                    grid.set_state(s)\n",
    "                    r = grid.move(a)\n",
    "                    s_prime = grid.current_state()\n",
    "                    new_v += p * (r + gamma * V[s_prime])\n",
    "                V[s] = new_v\n",
    "                delta = max(delta, V[s] - old_v)\n",
    "        if delta < THRESHOLD:\n",
    "            break\n",
    "    \n",
    "    # Perform Policy Improvement\n",
    "    changed = False\n",
    "    for s in states:\n",
    "        if s in grid.actions:\n",
    "            old_a = policy[s]\n",
    "            possible_actions = grid.actions[s]\n",
    "            best_a = old_a\n",
    "            for a in possible_actions:\n",
    "                v = 0\n",
    "                for a2 in POSSIBLE_ACTIONS:\n",
    "                    if a2 == a:\n",
    "                        p = 0.5\n",
    "                    else:\n",
    "                        p = 0.5 / 3\n",
    "                    grid.set_state(s)\n",
    "                    r = grid.move(a2)\n",
    "                    s_prime = grid.current_state()\n",
    "                    v += p * (r + gamma * V[s_prime])\n",
    "                best_a = a if v > V[s] else best_a\n",
    "            policy[s] = best_a\n",
    "            if best_a != old_a:\n",
    "                changed = True\n",
    "\n",
    "print('\\nValue function: ')\n",
    "print_values(V, grid)\n",
    "print('\\nOptimal Policy:')\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "We merge the policy evaluation and the policy improvement steps. We have only one Bellmann Equation that takes the max value of V(s) over all the actions.\n",
    "$$ V_{k+1}(s) = \\max_a{\\sum_{s'}\\sum_{r} p(s', r | s, a)[r + \\gamma V_k(s')]}$$\n",
    "It's more efficient because we have no longer an iterative algorithm (Iterative Policy Evaluation) inside an iterative algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "\n",
      "-----------------------\n",
      "-0.10 -0.10 -0.10  1.00 \n",
      "-----------------------\n",
      "-0.10  0.00 -0.10 -1.00 \n",
      "-----------------------\n",
      "-0.10 -0.10 -0.10 -0.10 \n",
      "-----------------------\n",
      "Random policy:\n",
      "---------------\n",
      " D   L   R      \n",
      "---------------\n",
      " D       R      \n",
      "---------------\n",
      " U   L   R   U  \n",
      "---------------\n",
      "\n",
      "\n",
      "Value function: \n",
      "-----------------------\n",
      " 0.62  0.80  1.00  0.00 \n",
      "-----------------------\n",
      " 0.46  0.00  0.80  0.00 \n",
      "-----------------------\n",
      " 0.31  0.46  0.62  0.46 \n",
      "-----------------------\n",
      "\n",
      "Optimal Policy:\n",
      "---------------\n",
      " R   R   R      \n",
      "---------------\n",
      " U       U      \n",
      "---------------\n",
      " U   R   U   L  \n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "from grid_world import negative_grid\n",
    "\n",
    "POSSIBLE_ACTIONS = ['U', 'D', 'L', 'R']\n",
    "gamma = 0.9\n",
    "THRESHOLD = 10e-4\n",
    "\n",
    "grid = negative_grid()\n",
    "print('Rewards:\\n')\n",
    "print_values(grid.rewards, grid)\n",
    "states = grid.all_states()\n",
    "\n",
    "policy = {s: grid.actions[s][np.random.choice(len(grid.actions[s]))] for s in states if s in grid.actions}\n",
    "V = {s: 0 for s in states}\n",
    "print('Random policy:')\n",
    "print_policy(policy, grid)\n",
    "print()\n",
    "\n",
    "while True:\n",
    "    delta = 0\n",
    "    for s in states:\n",
    "        old_v = V[s]\n",
    "        new_v = 0\n",
    "        max_v = 0\n",
    "        if s in grid.actions:\n",
    "            for a in grid.actions[s]:\n",
    "                grid.set_state(s)\n",
    "                r = grid.move(a)\n",
    "                s_prime = grid.current_state()\n",
    "                new_v = 1 * (r + gamma * V[s_prime])\n",
    "                max_v = max_v if max_v >= new_v else new_v\n",
    "            V[s] = max_v\n",
    "            delta = max(delta, V[s] - old_v)\n",
    "    if delta < THRESHOLD:\n",
    "        break\n",
    "        \n",
    "# Find the optimal policy from the optimal value function\n",
    "for s in policy.keys():\n",
    "    max_a = None\n",
    "    max_v = float('-inf')\n",
    "    for a in grid.actions[s]:\n",
    "        grid.set_state(s)\n",
    "        r = grid.move(a)\n",
    "        s_prime = grid.current_state()\n",
    "        v = 1 * (r + gamma * V[s_prime])\n",
    "        if v > max_v:\n",
    "            max_a = a\n",
    "            max_v = v\n",
    "    policy[s] = max_a\n",
    "\n",
    "print('\\nValue function: ')\n",
    "print_values(V, grid)\n",
    "print('\\nOptimal Policy:')\n",
    "print_policy(policy, grid)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
